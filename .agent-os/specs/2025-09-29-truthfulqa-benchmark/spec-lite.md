# Spec Summary (Lite)

Implement the TruthfulQA benchmark using multiple-choice evaluation tasks (MC1 and MC2) to measure model truthfulness across 817 questions spanning 38 categories. This implementation uses log-probability evaluation requiring no fine-tuned judge models, following the standard approach used by major LLM benchmarks like HuggingFace Open LLM Leaderboard.
